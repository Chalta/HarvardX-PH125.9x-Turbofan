
---
title: |
    | Machine Learning for Predictive Maintenance: 
    | A Prognostic Model to Estimate End-of-Life of Turbofan Aircraft Engines

author: ""
date: "June 10, 2019"
output:
  pdf_document:
    dev: png
    toc: true
    toc_depth: 2
urlcolor: orange

---



```{r Install Packages, include=FALSE}


#Note: This script may take a while to run. 
      #It took just over 20 minutes on a machine with the below specifications:
      #CPU: Intel(R) Core(TM) i5-7300U CPU @ 2.60GHz, 2712 Mhz, 2 Core(s), 4 Logical Processor(s)
      #RAM: 8.00 GB


#Github repository here: https://github.com/Chalta/HarvardX-PH125.9x-Turbofan


##Installing Packages

# List of packages for session
.packages = c("tidyverse",       #tidy alvvays and forever!
              "car",             #for calculating variance inflation factor
              "caret",           #Classification And REgression Training
              "corrplot",        #correlation plots
              "cowplot",         #solve x-axis misalignment when plotting, and better-looking defaults for ggplots
              "factoextra",      #visualize PCA
              "GGally",          #Pairs plots
              "gridExtra",       #combine plots
              "knitr",           #report output
              "kableExtra",      #nice tables
              "reshape2",        #acast to create matrix
              "scales"           #percent_format, among other things
              )


# Install CRAN packages (if not already installed)
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])

# Load packages into session 
lapply(.packages, require, character.only=TRUE)


```

```{r Functions and Hooks, include=FALSE}


#Set default chunk behaviour to *not* show code. We will only show code and charts we wish to.
knitr::opts_chunk$set(echo=FALSE)

#Set Thousands Separator for inline output
knitr::knit_hooks$set(inline = function(x) { if(!is.numeric(x)){ x }else{ prettyNum(round(x,2), big.mark=",") } })

#we've already set the graphic device to "png" in the RMD options. the default device for pdfs draws every point of a scatterplot, creatinvg *very* big files.
#But png is not as crisp, so we will set a higher resolution for pdf output of plots. 
knitr::opts_chunk$set(dpi=300)

#Create Kable wrapper function for thousands separator in table output, and nice formating with kableExtra
niceKable = function(...) {
  knitr::kable(..., format.args = list(decimal.mark = '.', big.mark = ",")) %>% kable_styling()
}

# turn off scientific display
options(scipen=999) 


#Make pretty confusion matrices in ggplot2
ggplotConfusionMatrix <- function(m){
  mytitle <- paste("Accuracy", percent_format()(m$overall[1]),
                   "Kappa", percent_format()(m$overall[2]))
  p <-
    ggplot(data = as.data.frame(m$table) ,
           aes(x = reorder(Reference, desc(Reference)), 
               y = reorder(Prediction, desc(Prediction)))) +
    xlab( "Reference") +
    ylab("Prediction") + 
    geom_tile(aes(fill = log(Freq)), colour = "white") +
    scale_fill_gradient(low = "white", high = "#8EB4C0") +
    geom_text(aes(x = Reference, y = Prediction, label = Freq)) +
    theme(legend.position = "none") +
    ggtitle(mytitle)
  return(p)
}



```

# Introduction

This report will outline a machine learning method to predict remaining useful life (RUL) of commercial turbofan engines, based on a dataset containing values from multiple sensors and operational settings. The data set was provided by the Prognostics CoE at NASA Ames:

> The Modular Aero-Propulsion System Simulation (MAPSS) is a flexible turbofan engine simulation environment that provides easy access to health, control, and engine parameters. 

> Both military and commercial turbofan engine versions of MAPSS exist. The commercial versions, C-MAPSS and C-MAPSS40k, represent high-bypass engines capable of 90,000 lbf thrust and 40,000 lbf thrust, respectively. 

> Data sets consists of multiple multivariate time series. Each data set is further divided into training and test subsets. Each time series is from a different engine â€“  the data can be considered to be from a fleet of engines of the same type. Each engine starts with different degrees of initial wear and manufacturing variation which is unknown to the user. This wear and variation is considered normal, i.e., it is not considered a fault condition. 
There are three operational settings that have a substantial effect on engine performance. These settings are also included in the data. The data is contaminated with sensor noise.

> Also provided [is] a vector of true Remaining Useful Life (RUL) values for the test data.

```{r}

#Set the data. Source is here: https://ti.arc.nasa.gov/c/6/
#Column names are derived from the Readme included with the data. 

#The real names of the settings and Sensors can be found in this article, which itself references original article by Saxena and Simon, both cited at the end of this document.

#https://pdfs.semanticscholar.org/69eb/732555d5ce743dc2e598384ea9d0e77fbbf4.pdf

colNames = c("unit", 
            "cycle",
            "setting1",
            "setting2",
            "setting3",
            "sensor1",
            "sensor2",
            "sensor3",
            "sensor4",
            "sensor5",
            "sensor6",
            "sensor7",
            "sensor8",
            "sensor9",
            "sensor10",
            "sensor11",
            "sensor12",
            "sensor13",
            "sensor14",
            "sensor15",
            "sensor16",
            "sensor17",
            "sensor18",
            "sensor19",
            "sensor20",
            "sensor21"
            )


test <- read.table("test_FD001.txt", col.names = colNames)
trueRUL <- read.table("RUL_FD001.txt", col.names = "RUL")
train <- read.table("train_FD001.txt", col.names = colNames)

```

RUL is the target variable. Instead of treating this as a regression problem and estimating for RUL directly, this project will define an engine health status and classify engines as operating in one of three statuses: "Normal", "Monitor" or "Critical".  Inspection or maintenance activities would be advisable for any engine that reaches the "Monitor" status. 

After normalizing the data, a random forest model will be fit to the data.

# Methods/Analysis

This analysis will be performed on the *FD001* test and train sets, and the associated vector of RUL values for the test set.

## Exploratory Data Analysis

There are no null values in the provided Test, Train or RUL datasets.

```{r}

data.frame("Train"= anyNA(train), "Test" = anyNA(test), "RUL"= anyNA(trueRUL)) %>% niceKable()
```




```{r include=FALSE}
#Check individual columns for NAs
#In this case, not necessary
sapply(test, {function(x) any(is.na(x))})
sapply(train, {function(x) any(is.na(x))})
sapply(trueRUL, {function(x) any(is.na(x))})
```

The training data appears as follows.

```{r echo=FALSE}

# Print a nice, truncated table


train_view <- train %>% select(-everything(), unit, cycle, setting1, setting2, setting3, sensor1, sensor2, sensor3, sensor4, sensor21)  %>% filter(row_number() < 6 | row_number() >n() - 6) %>%  rename("..." = sensor4)
train_view[6,] = "..."
train_view %>% kable %>% kable_styling()


```

NASA, defines the "remaining useful life" for each engine as below:

> The engine is operating normally at the start of each time series, and develops a fault at some point during the series.  In the training set, the fault grows in magnitude until system failure. In the test set, the time series ends some time prior to system failure.  

Therefore, for the training set, "remaining useful life" will be calculated for each engine, ending at 0 on the final cycle. 


```{r}

# Add remaining useful life, counting down from first cycle to end on 0 at last cycle (failure)
train1 <- train %>% 
    #mutate(unit = as.factor(unit)) %>%
    group_by(unit) %>% 
    mutate(RUL = n() - row_number() )

#Add ground truth RUL to test set
test1 <- test %>% 
  #filter(unit == 2) %>%
  group_by(unit) %>%
  mutate(RUL = trueRUL$RUL[unit] + n() - row_number() )

#test$RUL
#trueRUL$RUL[2]
```


```{r echo=FALSE}

# Print a nice, truncated table with RUL values.
train_view <- train1
train_view$sensor2 = "..."
train_view[6,] ="..."
train_view <- train_view %>% select(-everything(), unit, cycle, setting1, setting2, setting3, sensor1, sensor2, RUL) %>% rename("..." = sensor2)

train_view[c(1:6,190:192),]  %>% kable %>% kable_styling()
```

```{r}
# Count how many cycles in engine 1, just for easier embedded number in paragraph below
test_engine1cycles <- test1 %>% group_by(unit) %>% filter(unit == 1) %>% ungroup() %>% count()
test_engine1cycles <- as.numeric(test_engine1cycles) - 1
```

For the test set, the "RUL" vector of values will be added to each engine on the final cycle, and then iteratively increased for each cycle prior. For example, Engine 1 had `r trueRUL[1,]` cycles of "remaining useful life" at the point where the test data ends. With only `r test_engine1cycles` cycles recorded for Engine 1, it will therefore start with 142 cycles of "remaining useful life".

```{r}

test_view <- test1
test_view[6,] ="..."
test_view <- test_view %>% select(-everything(), unit, cycle, setting1, setting2, setting3, sensor1, sensor2, RUL) %>% rename("..." = sensor2)

test_view[c(1:6,30:31),]  %>% kable %>% kable_styling()


```

There are 100 engines in each dataset. As the chart below indicates, the *Test* engines contain fewer operating cycles. 

```{r fig.height = 2.5}


#Distributon of cycles in TRAIN
trainCycles <- train %>% group_by(unit) %>% 
  summarize(cycles = max(cycle)) %>%
  ggplot(aes  (x= reorder(unit, desc(cycles)), y=cycles ) ) + 
    ggtitle("Train Set") + 
  xlab("Unit") +
  geom_bar(stat = "identity", fill = "#3F7F93")+
  ylim(0, max(test$cycle, train$cycle)) + #set chart y-axis to maximum cycle in either test or train
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

#Distributon of cycles in TEST
testCycles <- test %>% group_by(unit) %>% 
  summarize(cycles = max(cycle)) %>%
  ggplot(aes  (x= reorder(unit, desc(cycles)), y=cycles ) ) + 
  ggtitle("Test Set") + 
  xlab("Unit") +
  geom_bar(stat = "identity", fill = "#8EB4C0") +
  ylim(0, max(test$cycle, train$cycle)) + #set chart y-axis to maximum cycle in either test or train
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())



grid.arrange(ggplotGrob(trainCycles), ggplotGrob(testCycles), ncol = 2)

```
The test set is put away until it is used later to validate the model.

\newpage

## Sensor Visualization
The next figure displays a visual exploration of the sensor and settings values for each predictor in every engine of the training set. The chart is aligned "to the right" on the RUL values. In the training set, each engine is run until failure, and the right-most point on the x-axis is "RUL = 0" - the point of failure. This visualization accounts for the fact that not all engines have an equal number of cycles. Some sensors show a clear trend as the engine approaches failure.


```{r fig.height = 7}

plotTrain <- train1 %>%
#  filter(unit == 1) %>%
  ungroup() %>% 
  select(-unit, - cycle) %>%
  melt(id="RUL")


  ggplot(plotTrain,aes(x=RUL,y=value)) + 
  facet_wrap(~ variable, scales="free_y", ncol = 3) + 
  geom_line(alpha = 0.5, colour = "#8EB4C0")+
  scale_x_reverse() 
  # theme(strip.text.x = element_text(size = 8)) 
  


```

## Normalization / Standardization

The previous figure demonstrated clearly that the predictor values are not normalized, and some contain no meaningful content.  The values for each setting and sensor will be pre-processed using three methods:

* *Scaling* the data to an interval between 0 and 1, by dividing the values by the standard deviation, and 
* *Centering* the values, by subtracting the mean, and 
* Removing predictors with *near-zero variance*

The below output shows which predictors were centered, scaled, and removed.

```{r}

train_norm <- train1
test_norm <- test1 


#NORMALIZE THE VARIABLES


#Develop preprocess parameters on training data, excluding categorical columns 1-2 (unit and cycle), and 27 (RUL)
#Center and scale values, remove paramaters with "near-zero variation"

preProcValues <- preProcess(train1[3:26], method = c("center", "scale", "nzv"))

#Apply the same preprocess parameters to transform both train *and* test into normalized values
train_norm <- predict(preProcValues, train1)
test_norm <- predict(preProcValues, test1)


preProcValues$method
```

The same parameters used in pre-processing the training set will be applied to pre-processing the test set.

As the next figure shows, the data is now normalized:

```{r fig.height = 5}

plotTrain <- train_norm %>%
#  filter(unit == 1) %>%
  ungroup() %>% 
  select(-unit, - cycle) %>%
  melt(id="RUL")


  ggplot(plotTrain,aes(x=RUL,y=value)) + 
  facet_wrap(~ variable, scales="free_y", ncol = 3) + 
  geom_line(alpha = 0.5, colour = "#8EB4C0")+
  scale_x_reverse() 
  # theme(strip.text.x = element_text(size = 8)) 
  


```
\newpage

## Predictor Correlation

A simple linear model is generated and then used to check for multicollinearity in the predictors, by computing the *Variance Inflation Factor*:

```{r}
simple_lm <- lm(RUL ~ ., data = train_norm)
vif(simple_lm) %>% niceKable

```

Sensors 9 and 14 have a VIF that is quite high relative to the rest of the data, indicating strong multicollinearity. Sensor 9 records the "Physical core speed (rpm)", whereas Sensor 14 records the "Corrected core speed (rpm)" (Saxena and Simon, 2008). "Corrected speed" adjusts component rotation to ambient conditions at sea level.  A correlation plot may assist in determining which sensor to keep.


```{r fig.height= 3.5}

train_cor <- train_norm %>% ungroup() %>% select(-unit, -cycle)

corrplot(cor(train_cor), method = "square", type="upper")


```

The remaining operational *settings* have very little correlation with RUL. Setting 1 (Altitude) and Setting 2 (Mach Number) will be removed from the data.

The remaining *sensors* are strongly correlated with each other, and most also have a strong correlation with RUL. Interestingly, Sensors 9 and 14 both have low correlation with the other sensors and with RUL. (As expected, they both have a strong correlation with each other.)

But Sensor 9 has a stronger correlation with RUL, so Sensor 14 will be dropped, and the VIF re-computed.

```{r}


train_norm <- train_norm %>% select(-setting1, -setting2, -sensor14)  #Training data
test_norm  <- test_norm  %>% select(-setting1, -setting2, -sensor14)  #Do the same for test

simple_lm <- lm(RUL ~ ., data = train_norm)
vif(simple_lm) %>% niceKable()

```



```{r pairs plot, eval=FALSE, include=FALSE}

#Generate a pairs plot from the remaining predictors.
#Couldn't make it look nice for the PDF report. So it was reluctantly cut.
#But preserved here.

train_pairs <- train_norm %>% ungroup() %>% select(-unit, -cycle) %>% ggpairs(., progress = FALSE) 
train_pairs
```

## Feature Engineering

Three engine health statuses are added to the dataset. An engine will be flagged "Critical" when it has 15 or fewer cycles remaining. An engine will be flagged "Monitor" at 40 or fewer cycles, and "Normal" in all other cases.


```{r}

# Create test and train sets

train_x    <- train_norm   %>% ungroup() %>%
              select(-unit, -cycle, -RUL)
                                        

train_y <-    train_norm %>% ungroup() %>%
              mutate(status = case_when(
                     RUL <= 15  ~ "Critical",
                     RUL <= 40  ~ "Monitor",
                     RUL >  40 ~ "Normal"),
                     status = as.factor(status)) %>% 
              select(status)


test_x    <- test_norm   %>% ungroup() %>%
              select(-unit, -cycle, -RUL)
                                        

test_y <-     test_norm %>% ungroup() %>%
              mutate(status = case_when(
                     RUL <= 15  ~ "Critical",
                     RUL <= 40  ~ "Monitor",
                     RUL >  40 ~ "Normal"),
                     status = as.factor(status)) %>% 
              select(status)

```


```{r}

#===========================================
# Plot Nice View of Engine Health Over Time
#===========================================

train_x_comp <- train_norm %>% ungroup()
train_y_comp <- train_y %>% mutate(status = as.factor(status))
train_complete <- cbind(train_x_comp, train_y_comp)


  train_complete %>% 
  ggplot(aes(x=1, y=cycle, fill=status)) + 
  facet_wrap(~ unit, ncol = 4, strip.position="left") + 
  ggtitle("Engine Health in The Training Set") +
  geom_bar(stat = "identity") +
  scale_fill_manual(values=c("#c3553a", "#DB9A8A", "#8EB4C0")) +
  xlab("Unit") +
  coord_flip() +
  theme(strip.background = element_blank(),
    strip.text.y = element_text(angle = 180, size = 8),
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    axis.line = element_blank()  
    )

```




## Modeling

```{r}
# Number of Folds
k = 5 

```

A random forest was fit to the test data using the "ranger" method from the "caret" package. Cross-validation was completed using `r k` folds.

```{r random forest model, message=FALSE, warning=FALSE, include=FALSE}


# Build train control...
fitControl <- trainControl(method = "CV",
                           number = k,
                           verboseIter = TRUE)

#...and tuning grid
tgrid <- expand.grid(
  .mtry = 2:6,
  #.splitrule = "extratrees",
  .splitrule = c("gini", "extratrees"),
  .min.node.size = c(10, 20)
)

# Fit a random forest model using caret and the "ranger" package.

fit <- 
             train(x = train_x,
             y = train_y$status,
             method = "ranger", 
               tuneGrid = tgrid,
               trControl = fitControl,
               importance = "permutation"
             )


```

\newpage

# Results

The following chart shows the combinations of "minimal node size", "splitting rule", and "mtry" (number of variables split at each node) used in fitting the model. 

```{r echo=FALSE, fig.height=3}
plot(fit)
```

The best accuracy was achieved by choosing the `splitting rule` of `r as.character(fit$bestTune$splitrule)`, an `mtry` value of `r fit$bestTune$mtry` and a node size of `r fit$bestTune$min.node.size`.

## Variable Importance

Based on the "variable importance plot", the following predictors were most meaningful in the prediction.

```{r echo=FALSE, fig.height=3}
plot(varImp(fit))
```

\newpage

## Confusion Matrix

The model achieved performance that is described by the following statistics.

```{r}
predict_train <- predict(fit, train_x)
predict_test  <- predict(fit, test_x)


cm_train <- confusionMatrix(train_y$status, predict_train)
cm_test  <- confusionMatrix(test_y$status, predict_test, positive = "Critical")

cm_test

```


```{r echo=FALSE, fig.height = 3}
ggplotConfusionMatrix(cm_test)

```


```{r include=FALSE}

#=========================================================
# Create Datasets with Ground Truth vs. Predicted States
#=========================================================

test_x_comp <- test_norm %>% ungroup()
test_y_comp <- test_y %>% mutate(status = as.factor(status))
test_complete <- cbind(test_x_comp, test_y_comp)



pred_y_comp <- predict_test
pred_complete <- cbind(test_x_comp, pred_y_comp)
pred_complete <- pred_complete %>% mutate(status = pred_y_comp) %>% select(-pred_y_comp)


```


```{r eval=FALSE, include=FALSE}

#=========================================================
# Detail of Sensor 11 In Ground Truth vs. Predicted
#=========================================================

  ggplot(test_complete,aes(x=RUL,y=sensor11, colour=status)) + 
  geom_line()+
  scale_x_reverse() +
  theme(strip.text.x = element_text(size = 150)) +
  colScale

  ggplot(pred_complete,aes(x=RUL,y=sensor11, colour=status)) + 
  geom_line()+
  scale_x_reverse() +
  theme(strip.text.x = element_text(size = 150)) +
  colScale
  
  
```

\newpage

### True Test Values
```{r}

  test_complete %>% 
  ggplot(aes(x=1, y=cycle, fill=status)) + 
  facet_wrap(~ unit, ncol = 4, strip.position="left") + 
  geom_bar(stat = "identity") +
  scale_fill_manual(values=c("#c3553a", "#DB9A8A", "#8EB4C0")) +
  xlab("unit") +
  coord_flip() +
  theme(strip.background = element_blank(),
    strip.text.y = element_text(angle = 180, size = 8),
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    axis.line = element_blank()  
    )

```

## True vs. Predicted Values

The figures on this page and the next show the "true" vs. "predicted" statuses for each of the 100 engines in the test set. They can be compared by "flip booking" between the two pages.

\newpage

### Predicted Values
```{r}

  pred_complete %>%
  ggplot(aes(x=1, y=cycle, fill=status)) + 
  facet_wrap(~ unit, ncol = 4, strip.position="left") + 
  geom_bar(stat = "identity") +
  scale_fill_manual(values=c("#c3553a", "#DB9A8A", "#8EB4C0")) +
  xlab("unit") +
  coord_flip() +
  theme(strip.background = element_blank(),
    strip.text.y = element_text(angle = 180, size = 8),
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    axis.line = element_blank()  
    )


```


# Conclusion

In the test set, no engine was run until failure. The engine that was closest to failure had only `r min(trueRUL)` cycles remaining.

The random forest model correctly classified the majority of engine states. When detecting faults in commercial turbofan aircraft engines, the worst kind of error is a False Negative - classifying an engine as free of fault conditions, when it is not.  While False Positives may cause maintenance or inspection earlier than necessary, this is still preferable to an engine failing prematurely.

In the test set, all engines that were truly critical were classified in either the "Critical" or "Monitor" state, with the majority being correctly classified as "Critical". Engines in the "monitor" state would ideally be inspected and/or maintained prior to reaching the "Critical" state or failing entirely.

Importantly, no engines in the "Critical" state were predicted to be in the "Normal" state, and likewise no engines in the "Normal" state were predicted to be in the "Critical" state.

## Future Considerations

While not explored here, a neural network "Long-Short-Term Memory" (LSTM) approach may achieve improved performance, particularly with respect to improving sensitivity to the "Critical" status. Another option would be to treat this as a regression problem and try to estimate RUL directly. Methods of denoising or smoothing the data may also result in improvement.

# Citations

A. Saxena and K. Goebel (2008). "Turbofan Engine Degradation Simulation Data Set", NASA Ames Prognostics Data Repository (http://ti.arc.nasa.gov/project/prognostic-data-repository), NASA Ames Research Center, Moffett Field, CA

A. Saxena and D. Simon (2008). "Damage propagation modeling for aircraft engine run-to-failure simulation", International Conference on Prognostics and Health Management", October 6-9, 2008, Denver, CO.

A. Jain, P. Kundu and B. Lad (2014). "Prediction of Remaining Useful Life of an Aircraft Engine under Unknown Initial Wear", 5th International & 26th All India Manufacturing Technology, Design and Research Conference (AIMTDR 2014)

"Modular Aero-Propulsion System Simulations - MAPSS, C-MAPSS, C-MAPSS40k", Nasa Intelligent Control and Autonomy Branch (https://www.grc.nasa.gov/www/cdtb/software/mapss.html), NASA Glenn Research Center